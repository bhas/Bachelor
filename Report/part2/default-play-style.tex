\section{Learning a default strategy}
\label{sec:part2}
In the previous chapter we created a calculator which can calculate the probability of winning with any hole cards in any game state. We will use the calculator in this section to estimate the strength of our hole cards.\\

Before we can learn our poker bot to adapt to opponents strategy we first need it to learn a default one. When the poker bot first joins a poker game it has no information about the opponent. In this case it must use a default strategy while it gathers more information.

In this chapter we will find a solution to the problem statement:

\vspace{4mm}
\begin{statementBox2}{Problem statement 2}
How can we develop a default strategy without having information about the opponents?
\end{statementBox2}
\vspace{4mm}

Because the default strategy has to work against any type of opponent, we don't expect it to be able to win against every type of opponent. The focus of the default strategy is not to make the poker bot win, although that would be preferable, its goal is instead to reduce the loses while the system gathers information about the opponent. But how can we make sure that we create a bot that plays on a sufficient level that it wont loose unnecessary?

\subsection{Design}
To develop a default strategy we have two options.

The first option is to directly program how the computer shall act in all situations. In this case the programmers need a deep insight in how to play poker in order to decide what decisions are optimal in the different situations. One can also use the expertise of professional poker players in case one lack the insight.

The second option is to develop an self-learning algorithm for the computer. Such an algorithm uses the concept \textit{watch and learn} by observing other players playing poker and trying to learn the strategy behind the decisions. This method requires that the computer has something to observe.

Since we do not have any particular insight in how to play poker and do not have expertise from any pro poker players, we will implement a self-learning algorithm. Another advantage by using this method is that the computer is not limited by our understanding of the game instead it might end up learning things we did not know beforehand.

The University of Alberta has a research group that specializes in field of artificial intelligence in poker. They have released a dataset containing data from ~18.000 real-life rounds of Texas hold'em limit poker. The dataset only contains the hole cards of the players who make it to the showdown, so there is no data about the hole cards of the players who folds. This data will be used to develop the default strategy.\\

To implement the self-learning algorithm we use an artificial neural networks (ANN), see section \ref{sec:nn}. Our algorithm needs to recognise patterns in the players decisions and ANNs are very good at this. 

\subsection{Artificial neural network}
\label{sec:nn}
An artificial neural network (ANN) inspired by the human brain. An ANN can take any number of inputs and return any number of outputs. 

An ANN is made up of neurons that are connected into a network. Each neuron takes a set of inputs and give one output. Each input has a weight that determines influence of the input. The neuron uses an input function to calculate the net input, usually the sum of all weighted inputs. A transfer function takes the net input as argument and produces the output.

\begin{figure}[H]
  \center
    \includegraphics[scale=0.5]{images/nn/neuron.png}
  \caption{Model of a neuron. \label{fig:neuron}}
\end{figure}

The network consists of three types of layers as shown in figure. An ANN will always have an input layer and an output layer but it can have any number of hidden layers.

The ANN can be trained using a training set of inputs and expected outputs (depending of the learning rule). The ANN starts with random values for all the weights. For each input and expected output it will then adjust the weight in order to get closer to the expected result.   

A supervised neural network is when the neural network has a targeted output. This means that when the neural network is trying to learn from the dataset we know what the system should output. Given the data from our dataset has the action of the players, a supervised learning neural network can be created, by targeting the output to the given action the player performed. To create a neural network a framework called Neuroph. Neuroph was used as it was well documented.\\

A perceptron is a simple neural network which has inputs and outputs, but no hidden layers in which calculations could be made. Each neuron has a weight that is an indicator on how much a given input means for the output of the network. The sum of each neurons weight is passed into a transfer function, that is a mathematical representation for constructing the curve that fits the data points best.
To see if a a simple neural network could solve our problem statement, a simple perceptron was created. The perceptron takes two inputs, and gives two outputs. The inputs are given by looking at each hand in the dataset that is visible and following that given players behaviour.
The first input being the probability, that the current player who the perceptron is looking at in the dataset, has the winning hand. The second input being how many opponents the current player are playing against. 
The two outputs tell us if the strategy of the current player is aggressive or defensive. 
The perceptron uses a sigmoid transfer function to give us us the desired output 1 or 0 as to whether the strategy of the current player is aggressive or defensive.
\input{unused/nn1.tex}
This approach did not seem to suit our needs as after training the neural network it did not make reasonable decisions. The neural network somewhat found a context between the number of opponents versus as to if we should be aggressive or not. The probability of the player having the winning hand did not weight as much as the number of opponents did. Therefore we chose to implement a more complex neural network as the results from the perceptron perhaps were not the most preferable decisions in the given situations.\\


A multi-layer-perceptron(will be refereed to as MLP) is closely related to our first described perceptron. The most simple structure is almost identical, but instead only having 2 layers, the input and output layers. The MLP can have multiple hidden layers inbetween the two layers of inputs and outputs.
The inputs in the MLP is taking the same as the earlier discussed perceptron, but also the chips the current player has, how much it will cost the player to call the current bid, and the total pot(a.k.a the profit) for that game state.
The MLP has 1 hidden layer with 2 hidden neurons. One hidden neuron for the probability of having the winning hand and the number of opponents. The second hidden neuron is for the chips, the cost and the pot.
As with the perceptron the sum of each weight in the MLP will be passed into a transfer function. In this case we chose to use a STEP function.

\input{unused/nn2.tex}



\subsection{Test}
To test our first simple neural network a.k.a the perceptron, we ran through x games in the dataset. The neural network is not able to get particular smarter when running through an additional set of entries as the data is from different players and x games should be sufficient for the neural network to gain some kind of knowledge on how to play as the goal of this strategy is not to have a bot that can beat the opponents but minimize the loses.
The neural network now had to learn from the dataset 
 


\subsection{Discussion}
When our artificial intelligence starts out by playing the games of poker it doesn't have any kind of information about the opponents. So to make sure that the bot wont just go keep throwing away the bank roll. Instead we wanted the bot to minimizes our losses so that we would still have a decent bank roll when we have gathered information about the opponents so that the bot could make qualified guesses at what move would be the most appropriate in terms of the current opponent. The default player was never meant to be on the same level as a human player, but humans are only in a slightly better position than the default bot. The human player can like the bot only see the hole and community cards, but a human is also able to make a profile of the bot in their head. This means that they can learn how the bot decides and exploit this. 
This is one of the reasons that we chose to go with a neural network. In a neural network we are able to of course train the network to make correct decisions based on the targeted output that we give it.
But as the game proceeds the neural network has an input which will weight when we have enough information about the opponents to shift our gameplay from the default play style to a more adaptive one.
The time that it takes a human player to learn about the bot and adapt to it, should be the same for the bot. So if we imagine a human player who is good enough to decipher the way the bot is playing and adapt to it. When the human player does that, the bot should also have started to change its ways. Slowly as the bot learn it will adapt more and more to the opponent so when we have enough information the bot will shift completely and disregard the default play style.
\subsection{Conclusion}